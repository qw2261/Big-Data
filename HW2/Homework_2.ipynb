{"nbformat_minor": 2, "cells": [{"source": "<h1 align=\"center\">EECS E6893 Big Data Analytics - Homework Assignment 2</h1>\n<h2 align=\"center\">Name: Qi Wang</h2>\n<h2 align=\"center\">UNI: qw2261</h2>", "cell_type": "markdown", "metadata": {}}, {"source": "## Question 1. Friendship Recommendation Algorithm", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "from pyspark import SparkConf, SparkContext\nimport pyspark\nimport sys\nfrom collections import defaultdict", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Configure Spark\nsc = pyspark.SparkContext.getOrCreate()\n# The directory for the file\nfilename = \"gs://homework0_qi/HW2/q1.txt\"", "outputs": [], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "def getData(sc, filename):\n    \"\"\"\n    Load data from raw text file into RDD and transform.\n    Hint: transfromation you will use: map(<lambda function>).\n    Args:\n        sc (SparkContext): spark context.\n        filename (string): hw2.txt cloud storage URI.\n    Returns:\n        RDD: RDD list of tuple of (<User>, [friend1, friend2, ... ]),\n        each user and a list of user's friends\n    \"\"\"\n    # read text file into RDD\n    data = sc.textFile(filename)\n\n    # TODO: implement your logic here\n    data = data.map(lambda line: line.split(\"\\t\"))\n    data = data.map(lambda line: (int(line[0]), [int(each) for each in line[1].split(',')] if len(line[1]) else []))\n    \n    return data", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "# Get data in proper format\ndata = getData(sc, filename)", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "# Show the data structure of data collected\nprint(data.take(1))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(0, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94])]\n"}], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "def mapFriends(line):\n    \"\"\"\n    List out every pair of mutual friends, also record direct friends.\n    Hint:\n    For each <User>, record direct friends into a list:\n    [(<User>, (friend1, 0)),(<User>, (friend2, 0)), ...],\n    where 0 means <User> and friend are already direct friend,\n    so you don't need to recommand each other.\n\n    For friends in the list, each of them has a friend <User> in common,\n    so for each of them, record mutual friend in both direction:\n    (friend1, (friend2, 1)), (friend2, (friend1, 1)),\n    where 1 means friend1 and friend2 has a mutual friend <User> in this \"line\"\n\n    There are possibly multiple output in each input line,\n    we applied flatMap to flatten them when using this function.\n    Args:\n        line (tuple): tuple in data RDD\n    Yields:\n        RDD: rdd like a list of (A, (B, 0)) or (A, (C, 1))\n    \"\"\"\n    user = line[0]\n    friends = line[1]\n    for i in range(len(friends)):\n        # Direct friend\n        # TODO: implement your logic here\n        yield((user, (friends[i], 0)))\n\n        for j in range(i+1, len(friends)):\n            # Mutual friend in both direction\n            # TODO: implement your logic here\n            yield((friends[i], (friends[j], 1)))\n            yield((friends[j], (friends[i], 1)))", "outputs": [], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "# Get set of all mutual friends\nmapData = data.flatMap(mapFriends).groupByKey()", "outputs": [], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "def findMutual(line):\n    \"\"\"\n    Find top 10 mutual friend for each person.\n    Hint: For each <User>, input is a list of tuples of friend relations,\n    whether direct friend (count = 0) or has friend in common (count = 1)\n\n    Use friendDict to store the number of mutual friend that the current <User>\n    has in common with each other <User> in tuple.\n    Input:(User1, [(User2, 1), (User3, 1), (User2, 1), (User3, 0), (User2, 1)])\n    friendDict stores: {User2:3, User3:1}\n    directFriend stores: User3\n\n    If a user has many mutual frineds and is not a direct frined, we recommend\n    them to be friends.\n\n    Args:\n        line (tuple): a tuple of (<User1>, [(<User2>, 0), (<User3>, 1)....])\n    Returns:\n        RDD of tuple (line[0], returnList),\n        returnList is a list of recommended friends\n    \"\"\"\n    # friendDict, Key: user, value: count of mutual friends\n    friendDict = defaultdict(int)\n    # set of direct friends\n    directFriend = set()\n    # initialize return list\n    returnList = []\n\n    # TODO: Iterate through input to aggregate counts\n    # save to friendDict and directFriend\n    \n    for each_friend in line[1]:\n        friendDict[each_friend[0]] += each_friend[1]\n        if each_friend[1] == 0:\n            directFriend.add(each_friend[0])\n    \n    result = sorted(friendDict.iteritems(), key = lambda x : x[1], reverse = True)\n\n    # TODO: Formulate output\n    result = sorted(friendDict.iteritems(), key = lambda x : x[1], reverse = True)\n    count = 0\n    \n    for each in result:\n        if each[0] not in directFriend and count < 10:\n            returnList.append(each[0])\n            count += 1\n\n    return (line[0], returnList)", "outputs": [], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "# For each person, get top 10 mutual friends\ngetFriends = mapData.map(findMutual)", "outputs": [], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "# Only save the ones we want\nwanted = [924, 8941, 8942, 9019, 49824, 13420, 44410, 8974, 5850, 9993]\nresult = getFriends.filter(lambda x: x[0] in wanted).collect()", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "# Visualize the result for check\nfor each in sc.parallelize(result).sortBy(lambda pair: pair[0]).collect():\n    print(each)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(924, [43748, 2409, 6995, 11860, 439, 15416, 45881])\n(5850, [5819, 5805, 5811, 5815, 5828, 5831, 5836, 30209, 13315, 13322])\n(8941, [8943, 8944, 8940])\n(8942, [8939, 8940, 8943, 8944])\n(8974, [12241, 8960, 8774, 6973, 8969, 8982, 8980, 8984, 8979, 8978])\n(9019, [9022, 317, 9023])\n(9993, [9991, 13134, 34485, 34642, 13877, 34299, 37941, 13478])\n(13420, [10469, 7651, 4736, 14264, 2101, 21869, 30403, 10500, 47366, 10532])\n(44410, [4231, 44462, 28779, 22553, 14907, 10328, 10370, 17032, 28332, 6318])\n(49824, [49846, 41581, 49786, 49788, 49789, 49814, 49819, 49834, 43382, 10760])\n"}], "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "sc.stop()", "outputs": [], "metadata": {}}, {"source": "## Question 2. Graph Analysis", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "# Install graphframes\n!pip install \"git+https://github.com/munro/graphframes.git@release-0.5.0#egg=graphframes&subdirectory=python\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\nRequirement already satisfied: graphframes from git+https://github.com/munro/graphframes.git@release-0.5.0#egg=graphframes&subdirectory=python in /opt/conda/anaconda/lib/python2.7/site-packages (0.5.0)\n"}], "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "from graphframes import *\nfrom pyspark import SQLContext\nimport os", "outputs": [], "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "# Configure Spark\nif not os.path.isdir(\"checkpoints\"):\n    os.mkdir(\"checkpoints\")\nconf = SparkConf().setMaster(\"local\").setAppName('connected components')\nsc = SparkContext(conf = conf)", "outputs": [], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "# Configure sqlcontext and directory\nsqlContext = SQLContext(sc)\nSparkContext.setCheckpointDir(sc, \"checkpoints\")", "outputs": [], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "# Get data in proper format\ndata = getData(sc, filename)", "outputs": [], "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "def getVertices(data, sqlContext):\n    \"\"\"\n    Get the vertices of the friends network\n\n    Args:\n        data: RDD: RDD list of tuple of (<User>, [friend1, friend2, ... ]), each user and a list of user's friends\n        sqlContext: SQLContext\n        \n    Returns:\n        vertice: ID DataFrame of all users\n    \"\"\"\n    \n    return sqlContext.createDataFrame(data.map(lambda line: (line[0], )), schema = [\"id\"])", "outputs": [], "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "vertices = getVertices(data, sqlContext)", "outputs": [], "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "vertices.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n| 15|\n| 16|\n| 17|\n| 18|\n| 19|\n+---+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "def friendRelationBuid(line):\n    '''\n    Get the egde of the friends network\n\n    Args:\n        line (tuple): a tuple of (<User1>, [(<User2>, 0), (<User3>, 1)....])\n        \n    Yields:\n        friend relation (tuple): a tuple of (friend1, friend2)\n    '''\n    user = line[0]\n    friends = line[1]\n    \n    for each_friend in friends:\n        yield (user, each_friend)", "outputs": [], "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "def getEdges(data, sqlContext):\n    edges = data.flatMap(friendRelationBuid)\n    return sqlContext.createDataFrame(edges, schema = [\"src\", \"dst\"])", "outputs": [], "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "edges = getEdges(data, sqlContext)", "outputs": [], "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "edges.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+---+\n|src|dst|\n+---+---+\n|  0|  1|\n|  0|  2|\n|  0|  3|\n|  0|  4|\n|  0|  5|\n|  0|  6|\n|  0|  7|\n|  0|  8|\n|  0|  9|\n|  0| 10|\n|  0| 11|\n|  0| 12|\n|  0| 13|\n|  0| 14|\n|  0| 15|\n|  0| 16|\n|  0| 17|\n|  0| 18|\n|  0| 19|\n|  0| 20|\n+---+---+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "# Build graph\ngraph = GraphFrame(vertices, edges)", "outputs": [], "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "result = graph.connectedComponents()", "outputs": [], "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "result.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+---------+\n| id|component|\n+---+---------+\n|  0|        0|\n|  1|        0|\n|  2|        0|\n|  3|        0|\n|  4|        0|\n|  5|        0|\n|  6|        0|\n|  7|        0|\n|  8|        0|\n|  9|        0|\n| 10|        0|\n| 11|        0|\n| 12|        0|\n| 13|        0|\n| 14|        0|\n| 15|        0|\n| 16|        0|\n| 17|        0|\n| 18|        0|\n| 19|        0|\n+---+---------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"source": "## (1). How many clusters/connected components in total for this dataset?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "cluster_num = result.select(\"component\").distinct().count()\nprint \"The NO. of clusters are\", cluster_num", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The NO. of clusters are 917\n"}], "metadata": {}}, {"source": "## (2). How many users in the top 10 clusters?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "result.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+---------+\n| id|component|\n+---+---------+\n|  0|        0|\n|  1|        0|\n|  2|        0|\n|  3|        0|\n|  4|        0|\n|  5|        0|\n|  6|        0|\n|  7|        0|\n|  8|        0|\n|  9|        0|\n| 10|        0|\n| 11|        0|\n| 12|        0|\n| 13|        0|\n| 14|        0|\n| 15|        0|\n| 16|        0|\n| 17|        0|\n| 18|        0|\n| 19|        0|\n+---+---------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "group_info = result.groupBy(\"component\").count().orderBy('count', ascending = False)", "outputs": [], "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "top_ten = group_info.head(10)", "outputs": [], "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "user_num_ten = 0\nfor each_row in top_ten:\n    user_num_ten += each_row[\"count\"]\n    print(\"Cluster %d has %d Users\" % (each_row['component'], each_row['count']))\nprint(\"\\nThere are %d users in the top 10 clusters\" %user_num_ten)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Cluster 0 has 48860 Users\nCluster 38403 has 66 Users\nCluster 18466 has 31 Users\nCluster 18233 has 25 Users\nCluster 18891 has 19 Users\nCluster 864 has 16 Users\nCluster 49297 has 13 Users\nCluster 19199 has 6 Users\nCluster 7658 has 5 Users\nCluster 22897 has 4 Users\n\nThere are 49045 users in the top 10 clusters\n"}], "metadata": {}}, {"source": "## (3). What are the user ids for the cluster which has 25 users?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "target_cluster = group_info.where(group_info['count'] == 25).select(\"component\").collect()", "outputs": [], "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "target_cluster = [row['component'] for row in target_cluster]", "outputs": [], "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": "target_userid = [row['id'] for row in result.where(result['component'].isin(target_cluster)).select('id').collect()]", "outputs": [], "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "print('The user ids for cluster which has 25 users are: (the cluster is %s)'%target_cluster)\nprint(target_userid)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The user ids for cluster which has 25 users are: (the cluster is [18233])\n[18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257]\n"}], "metadata": {}}, {"source": "## (4). A list of 10 important users and the most important one", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "page_rank = graph.pageRank(resetProbability = 0.15, tol = 0.01)", "outputs": [], "metadata": {}}, {"execution_count": 38, "cell_type": "code", "source": "page_rank", "outputs": [{"execution_count": 38, "output_type": "execute_result", "data": {"text/plain": "GraphFrame(v:[id: bigint, pagerank: double], e:[src: bigint, dst: bigint ... 1 more field])"}, "metadata": {}}], "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "ten_important_user = [row['id'] for row in page_rank.vertices.orderBy('pagerank', ascending = False).head(10)]", "outputs": [], "metadata": {}}, {"execution_count": 40, "cell_type": "code", "source": "print(\"The 10 important users are \\n\")\nprint(ten_important_user)\nprint('\\nThe most important one is %d' % ten_important_user[0])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The 10 important users are \n\n[10164, 15496, 14689, 24966, 7884, 934, 45870, 5148, 20283, 46039]\n\nThe most important one is 10164\n"}], "metadata": {}}, {"source": "## (5). Using different parameter settings for PageRank, is there any difference?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "# Using different parameters\npage_rank = graph.pageRank(resetProbability = 0.1, maxIter = 30)\nten_important_user = [row['id'] for row in page_rank.vertices.orderBy('pagerank', ascending = False).head(10)]\nprint(\"The 10 important users are \\n\")\nprint(ten_important_user)\nprint('\\nThe most important one is %d' % ten_important_user[0])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The 10 important users are \n\n[10164, 15496, 14689, 24966, 7884, 934, 45870, 20283, 46039, 14996]\n\nThe most important one is 10164\n"}], "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": "# Using different parameters\npage_rank = graph.pageRank(resetProbability = 0.5, maxIter = 30)\nten_important_user = [row['id'] for row in page_rank.vertices.orderBy('pagerank', ascending = False).head(10)]\nprint(\"The 10 important users are \\n\")\nprint(ten_important_user)\nprint('\\nThe most important one is %d' % ten_important_user[0])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The 10 important users are \n\n[10164, 15496, 14689, 24966, 5148, 38123, 7884, 934, 910, 44815]\n\nThe most important one is 10164\n"}], "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "# Using different parameters\npage_rank = graph.pageRank(resetProbability = 0.15, tol = 0.1)\nten_important_user = [row['id'] for row in page_rank.vertices.orderBy('pagerank', ascending = False).head(10)]\nprint(\"The 10 important users are \\n\")\nprint(ten_important_user)\nprint('\\nThe most important one is %d' % ten_important_user[0])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The 10 important users are \n\n[10164, 15496, 14689, 24966, 5148, 38123, 934, 7884, 910, 44815]\n\nThe most important one is 10164\n"}], "metadata": {}}, {"source": "** Based on the tests, **\n\nresetProbability = 0.15, tol = 0.01:\n\n[10164, 15496, 14689, 24966, 7884, 934, 45870, 5148, 20283, 46039]\n\nresetProbability = 0.1, maxIter = 30:\n\n[10164, 15496, 14689, 24966, 7884, 934, 45870, 20283, 46039, 14996]\n\nresetProbability = 0.5, maxIter = 30:\n\n[10164, 15496, 14689, 24966, 5148, 38123, 7884, 934, 910, 44815]\n\nresetProbability = 0.15, tol = 0.1:\n\n[10164, 15496, 14689, 24966, 5148, 38123, 934, 7884, 910, 44815]\n\n** we can determine that there are some differences for PageRank when using different parameter setting, which \nmight result from that the tolerance is too large to converge within the given iterations. So we should set small\ntolerance and larger iterations to make the convergence happen.**", "cell_type": "markdown", "metadata": {}}, {"source": "## (6) Why this user become the most important one? What are the possible reasons?", "cell_type": "markdown", "metadata": {}}, {"source": "The PageRank outputs a probability distribution which represents how likely a person/object will be selected randomly. So, the reason that this user become the most important one is that this user belongs to the largest cluster in the graph. Also, this user has the largest incoming and outcoming edges, from the edges the user will be as possible as much linked to other users, making him/her obtain the highest PageRank weight in the graph, like a important transportation center.\n", "cell_type": "markdown", "metadata": {}}, {"source": "## (7) PageRank Calculation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 60, "cell_type": "code", "source": "import numpy as np", "outputs": [], "metadata": {}}, {"execution_count": 89, "cell_type": "code", "source": "def checkTol(prev, cur):\n    for each in np.abs(prev - cur):\n        if each > tol:\n            return False\n    return True", "outputs": [], "metadata": {}}, {"execution_count": 107, "cell_type": "code", "source": "from copy import deepcopy\nPR = {'ID1': 0.2, 'ID2': 0.2, 'ID3': 0.2, 'ID4': 0.2, 'ID5':0.2}\nIn = {'ID1': ['ID2'], 'ID2': ['ID3', 'ID5'], 'ID3': ['ID1', 'ID2', 'ID4', 'ID5'], 'ID4': ['ID2'], 'ID5': ['ID1', 'ID2']}\nL = {'ID1': 2, 'ID2': 4, 'ID3': 1, 'ID4': 1, 'ID5': 2}\nN = 5\nd = 0.85\ntol = 0.1\n\nresult = []\nprev = np.array(PR.values())\ncount = 0\nwhile count == 0 or not checkTol(prev, np.array(PR.values())):\n    prev = np.array(PR.values())\n    cur_PR = deepcopy(PR)\n    for each_user in ['ID1', 'ID2', 'ID3', 'ID4', 'ID5']:\n        PR[each_user] = (1 - d) / N\n        for each_In in In[each_user]:\n            PR[each_user] = PR[each_user] + d * (cur_PR[each_In] / L[each_In])\n        PR[each_user] = round(PR[each_user], 4)\n    count += 1\n    result.append(deepcopy(PR))\n    print(\"Iteration \" + str(count))\n    print(PR)\n    print", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Iteration 1\n{'ID4': 0.0725, 'ID5': 0.1575, 'ID2': 0.285, 'ID3': 0.4125, 'ID1': 0.0725}\n\nIteration 2\n{'ID4': 0.0906, 'ID5': 0.1214, 'ID2': 0.4476, 'ID3': 0.2499, 'ID1': 0.0906}\n\nIteration 3\n{'ID4': 0.1251, 'ID5': 0.1636, 'ID2': 0.294, 'ID3': 0.2922, 'ID1': 0.1251}\n\nIteration 4\n{'ID4': 0.0925, 'ID5': 0.1456, 'ID2': 0.3479, 'ID3': 0.3215, 'ID1': 0.0925}\n\n"}], "metadata": {}}, {"execution_count": 108, "cell_type": "code", "source": "result", "outputs": [{"execution_count": 108, "output_type": "execute_result", "data": {"text/plain": "[{'ID1': 0.0725, 'ID2': 0.285, 'ID3': 0.4125, 'ID4': 0.0725, 'ID5': 0.1575},\n {'ID1': 0.0906, 'ID2': 0.4476, 'ID3': 0.2499, 'ID4': 0.0906, 'ID5': 0.1214},\n {'ID1': 0.1251, 'ID2': 0.294, 'ID3': 0.2922, 'ID4': 0.1251, 'ID5': 0.1636},\n {'ID1': 0.0925, 'ID2': 0.3479, 'ID3': 0.3215, 'ID4': 0.0925, 'ID5': 0.1456}]"}, "metadata": {}}], "metadata": {}}, {"source": "Except the code implementation, we can calculate this by hand:\nPR = {'ID1': 0.2, 'ID2': 0.2, 'ID3': 0.2, 'ID4': 0.2, 'ID5':0.2}\nIn = {'ID1': ['ID2'], 'ID2': ['ID3', 'ID5'], 'ID3': ['ID1', 'ID2', 'ID4', 'ID5'], 'ID4': ['ID2'], 'ID5': ['ID1', 'ID2']}\nL = {'ID1': 2, 'ID2': 4, 'ID3': 1, 'ID4': 1, 'ID5': 2}\n\nIteration 1:\nInitial PR = {'ID1': 0.2, 'ID2': 0.2, 'ID3': 0.2, 'ID4': 0.2, 'ID5':0.2}\nnew_PR[ID1] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0725\nnew_PR[ID2] = (1 - 0.85) + 0.85 * (PR[ID3] / L[ID3] + PR[ID5] / L[ID5]) = 0.285\nnew_PR[ID3] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / L[ID2] + PR[ID4] / L[ID4] + PR[ID5] / L[ID5]) = 0.4125\nnew_PR[ID4] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0725\nnew_PR[ID5] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / :[ID2]) = 0.1575\n\nAfter iteration 1, we have {'ID1': 0.0725, 'ID2': 0.285, 'ID3': 0.4125, 'ID4': 0.0725, 'ID5': 0.1575}\nCompare to original PR, we find someone in the differences between iteration 1 and original PR is larger than tolerance and we should continue on next iteration.\n\nIteration 2:\nInitial PR = {'ID1': 0.0725, 'ID2': 0.285, 'ID3': 0.4125, 'ID4': 0.0725, 'ID5': 0.1575}\nnew_PR[ID1] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0906\nnew_PR[ID2] = (1 - 0.85) + 0.85 * (PR[ID3] / L[ID3] + PR[ID5] / L[ID5]) = 0.4476\nnew_PR[ID3] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / L[ID2] + PR[ID4] / L[ID4] + PR[ID5] / L[ID5]) = 0.2499\nnew_PR[ID4] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0906\nnew_PR[ID5] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / :[ID2]) = 0.1214\n\nAfter iteration 2, we have {'ID1': 0.0906, 'ID2': 0.4476, 'ID3': 0.2499, 'ID4': 0.0906, 'ID5': 0.1214}\nCompare to last PR, we find someone in the differences between iteration 2 and last PR is larger than tolerance and we should continue on next iteration.\n\nIteration 3:\nInitial PR = {'ID1': 0.0906, 'ID2': 0.4476, 'ID3': 0.2499, 'ID4': 0.0906, 'ID5': 0.1214}\nnew_PR[ID1] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.1251\nnew_PR[ID2] = (1 - 0.85) + 0.85 * (PR[ID3] / L[ID3] + PR[ID5] / L[ID5]) = 0.294\nnew_PR[ID3] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / L[ID2] + PR[ID4] / L[ID4] + PR[ID5] / L[ID5]) = 0.2922\nnew_PR[ID4] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.1251\nnew_PR[ID5] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / :[ID2]) = 0.1636\n\nAfter iteration 3, we have {'ID1': 0.1251, 'ID2': 0.294, 'ID3': 0.2922, 'ID4': 0.1251, 'ID5': 0.1636}\nCompare to last PR, we find someone in the differences between iteration 3 and last PR is larger than tolerance and we should continue on next iteration.\n\nIteration 4:\nInitial PR = {'ID1': 0.1251, 'ID2': 0.294, 'ID3': 0.2922, 'ID4': 0.1251, 'ID5': 0.1636}\nnew_PR[ID1] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0925\nnew_PR[ID2] = (1 - 0.85) + 0.85 * (PR[ID3] / L[ID3] + PR[ID5] / L[ID5]) = 0.3479\nnew_PR[ID3] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / L[ID2] + PR[ID4] / L[ID4] + PR[ID5] / L[ID5]) = 0.3215\nnew_PR[ID4] = (1 - 0.85) + 0.85 * (PR[ID2] / L[ID2]) = 0.0925\nnew_PR[ID5] = (1 - 0.85) + 0.85 * (PR[ID1] / L[ID1] + PR[ID2] / :[ID2]) = 0.1456\n\nAfter iteration 4, we have {'ID1': 0.0925, 'ID2': 0.3479, 'ID3': 0.3215, 'ID4': 0.0925, 'ID5': 0.1456}]\nCompare to last PR, we find  the differences between iteration 3 and last PR are all smaller than tolerance and the convergence happen, and our result is the same as the code running.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.16", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}